{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KerasUsingHdnLayer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMEZmn2szRzYKUw0QlqYhNS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"cwhK0IB2szml"},"source":["import tensorflow as tf\n","import numpy as np\n","import torch.nn as nn\n","from tensorflow.keras.datasets import mnist\n","\n","batch_size = 128 #60000개를 한번에 행렬로 가져올 수 없으니\n","#128개씩 나눠가져옴\n","\n","#사용할 중간 레이어들. 256으로 되어있다.\n","#256으로 되어있을땐 확률이 91%로 나온다.\n","\n","#만일 히든레이어를 돌리지 않을 경우, 확률은 86%로 떨어지는 것을 알 수 있다.(softmax)\n","#또한, 히든레이어 노드개수를 1로 설정할 경우엔, 10%대로 대폭 감소한다.\n","nH1=256\n","nH2=256\n","nH3=256\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train.astype('float32'), x_test.astype('float32')\n","#데이터를 [총개수/784, 784]로 변경 \n","x_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])\n","x_train, x_test = x_train/255., x_test/255. #0~1사이로 변경\n","y_train, y_test = tf.one_hot(y_train, depth=10), tf.one_hot(y_test, depth=10)\n","\n","#numpy array나 list를 tensor dataset으로 변환\n","#이후, 데이터셋을 임의로 배치사이즈만큼 섞음\n","train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","train_data = train_data.repeat().shuffle(5000).batch(batch_size)\n","\n","#히든레이어 적용\n","#W는 가중치넣는 배열, b는 bias개수\n","class ANN(nn.Module) :\n","  def __init__(self):\n","    self.W_1 = tf.Variable(tf.random.normal(shape=[784, nH1]))\n","    self.W_2 = tf.Variable(tf.random.normal(shape=[nH1, nH2]))\n","    self.W_3 = tf.Variable(tf.random.normal(shape=[nH2, nH3]))\n","    self.W_Out = tf.Variable(tf.random.normal(shape=[nH3, 10]))\n","    #self.W_Out = tf.Variable(tf.random.normal(shape=[784, 10]))\n","    self.b_1 = tf.Variable(tf.random.normal(shape=[nH1]))\n","    self.b_2 = tf.Variable(tf.random.normal(shape=[nH1]))\n","    self.b_3 = tf.Variable(tf.random.normal(shape=[nH1]))\n","    self.b_Out = tf.Variable(tf.random.normal(shape=[10]))\n","    #self.b_Out = tf.Variable(tf.random.normal(shape=[10]))\n","\n","  def __call__(self, x) :\n","    H1_Out = tf.nn.relu(tf.matmul(x, self.W_1) + self.b_1)\n","    H2_Out = tf.nn.relu(tf.matmul(H1_Out, self.W_2) + self.b_2)\n","    H3_Out = tf.nn.relu(tf.matmul(H2_Out, self.W_3) + self.b_3)\n","    Out = tf.matmul(H3_Out, self.W_Out) + self.b_Out\n","    #Out = tf.matmul(x, self.W_Out) + self.b_Out\n","    return Out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C1czfBMiuaY6","executionInfo":{"status":"ok","timestamp":1638250162259,"user_tz":-540,"elapsed":3796,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}},"outputId":"feb221f7-d24e-4d8b-b221-460ad40e02ec"},"source":["ANN_model = ANN()\n","optimizer = tf.optimizers.Adam(0.01)\n","\n","#softmax 비용함수(비용 구하기) 상세한건 이전거 참조\n","def cost_ANN_mnist(x,y) :\n","  y = tf.cast(y, tf.int64)\n","  loss = tf.nn.softmax_cross_entropy_with_logits(logits=x, labels=y)\n","  return tf.reduce_mean(loss)\n","\n","#tf.argmax 비교 통해, 맞는지 확인\n","def accuracy(x,y) :\n","  correct = tf.equal(tf.argmax(x,1), tf.argmax(y,1))\n","  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","  return accuracy\n","\n","def train_optimization(x,y) :\n","  with tf.GradientTape() as g:\n","    pred = ANN_model(x) #Ann 모델에 x 데이터 삽입\n","    cost = cost_ANN_mnist(pred, y) #비용 구하기\n","  gradients = g.gradient(cost, vars(ANN_model).values(), unconnected_gradients=\n","                         tf.UnconnectedGradients.ZERO)\n","  #기존 gradients와 다르게 뒤에 unconnected...를 정의함으로써, 연결되지않는 그래디언트로 인한 \n","  #오류를 제거\n","  optimizer.apply_gradients(zip(gradients, vars(ANN_model).values()))\n","\n","#300개씩 가져와 batch_x, batch_y에 넣음\n","#섞인 train data들을, 300개(+y는 1개씩)가져와 batch_x, y에 넣음\n","for step, (batch_x, batch_y) in enumerate(train_data.take(300), 1):\n","  train_optimization(batch_x, batch_y)\n","  #batch_x 넣고, 러닝 돌린다. 이를 batch_y와 비교해, 손실 구하고\n","  #정확도 역시 판단.\n","  if step%10 == 0:\n","    pred = ANN_model(batch_x)\n","    loss = cost_ANN_mnist(pred, batch_y)\n","    acc = accuracy(pred, batch_y)\n","    print(\"Step: {}, \\t Accuracy: {}, \\t Loss:{}\".format(step, acc.numpy().flatten(),\n","                                                         loss.numpy().flatten()))\n","\n","print('='*30)\n","print('Model Ttest')\n","print(\"Test accuracy: \", accuracy(ANN_model(x_test), y_test).numpy().flatten())\n","print('=' * 30)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Step: 10, \t Accuracy: [0.0859375], \t Loss:[2.561834]\n","Step: 20, \t Accuracy: [0.1015625], \t Loss:[2.3733506]\n","Step: 30, \t Accuracy: [0.1171875], \t Loss:[2.473302]\n","Step: 40, \t Accuracy: [0.0859375], \t Loss:[2.3724947]\n","Step: 50, \t Accuracy: [0.0625], \t Loss:[2.381276]\n","Step: 60, \t Accuracy: [0.109375], \t Loss:[2.350648]\n","Step: 70, \t Accuracy: [0.125], \t Loss:[2.3014336]\n","Step: 80, \t Accuracy: [0.078125], \t Loss:[2.3593125]\n","Step: 90, \t Accuracy: [0.0625], \t Loss:[2.302682]\n","Step: 100, \t Accuracy: [0.0859375], \t Loss:[2.304474]\n","Step: 110, \t Accuracy: [0.109375], \t Loss:[2.324452]\n","Step: 120, \t Accuracy: [0.0546875], \t Loss:[2.3355286]\n","Step: 130, \t Accuracy: [0.09375], \t Loss:[2.3164272]\n","Step: 140, \t Accuracy: [0.109375], \t Loss:[2.307311]\n","Step: 150, \t Accuracy: [0.125], \t Loss:[2.292652]\n","Step: 160, \t Accuracy: [0.1328125], \t Loss:[2.2962117]\n","Step: 170, \t Accuracy: [0.0859375], \t Loss:[2.3022137]\n","Step: 180, \t Accuracy: [0.1328125], \t Loss:[2.2924647]\n","Step: 190, \t Accuracy: [0.1328125], \t Loss:[2.3039732]\n","Step: 200, \t Accuracy: [0.1328125], \t Loss:[2.3063297]\n","Step: 210, \t Accuracy: [0.078125], \t Loss:[2.3061829]\n","Step: 220, \t Accuracy: [0.125], \t Loss:[2.2934465]\n","Step: 230, \t Accuracy: [0.1015625], \t Loss:[2.3006587]\n","Step: 240, \t Accuracy: [0.1171875], \t Loss:[2.3002405]\n","Step: 250, \t Accuracy: [0.1171875], \t Loss:[2.3038354]\n","Step: 260, \t Accuracy: [0.1015625], \t Loss:[2.2996085]\n","Step: 270, \t Accuracy: [0.109375], \t Loss:[2.306713]\n","Step: 280, \t Accuracy: [0.1328125], \t Loss:[2.3006127]\n","Step: 290, \t Accuracy: [0.125], \t Loss:[2.2975316]\n","Step: 300, \t Accuracy: [0.1015625], \t Loss:[2.3076982]\n","==============================\n","Model Ttest\n","Test accuracy:  [0.1135]\n","==============================\n"]}]},{"cell_type":"code","metadata":{"id":"l0BLdxHvV0_K"},"source":["#p.102 Ann PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import confusion_matrix\n","from torch.autograd import Variable\n","\n","train_data =datasets.MNIST(root = '../Data', train = True, download = True,\n","                           transform = transforms.ToTensor())\n","test_data =datasets.MNIST(root = '../Data', train = True, download = True,\n","                           transform = transforms.ToTensor())\n","train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n","test_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n","\n","class ANNModel(nn.Module) :\n","  def __init__(self, input_dim, hidden_dim, output_dim):\n","    super(ANNModel, self).__init__()\n","    self.net = nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU(),\n","                             nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n","                             nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n","                             nn.Linear(hidden_dim, output_dim))\n","    \n","    def forward(self, x) :\n","      return self.net(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":396},"id":"1JtJ0xG7W4uW","executionInfo":{"status":"error","timestamp":1638252762544,"user_tz":-540,"elapsed":287,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}},"outputId":"edf942fc-7dbc-4571-da57-1e780c393b92"},"source":["model = ANNModel(input_dim = 784, hidden_dim = 256, output_dim = 10)\n","cost = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n","step = 0\n","loss_list = []\n","accuracy_list = []\n","\n","for epoch in range(5):\n","  for i, (images, labels) in enumerate(train_loader):\n","    train = Variable(images.view(-1, 784))\n","    labels = Variable(labels)\n","    optimizer.zero_grad()\n","    outputs = model(train)\n","    loss = cost(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    step += 1\n","    if step % 50 == 0:\n","      correct = 0\n","      total = 0\n","      for images, labels in test_loader:\n","        test = Vriable(images.view(-1, 784))\n","        outputs = model(test)\n","        predicted = torch.max(oututs.data, 1)[1]\n","        total += len(labels)\n","        correct += (predicted == labels).sum()\n","      accuracy = correct.item() / total\n","      loss_list.append(loss.data.item())\n","      accuracy_list.append(accuracy)\n","      if step%100 == 0:\n","        print(\"step:{}, \\tAccuracy:{}, \\t Loss:{}\".format(step, accuracy, loss))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NotImplementedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-7cdc0c9afcff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \"\"\"\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"_fo-hU57gB2G"},"source":["\"\"\"\n","Interpolation: p.170에서, W와 b를 찾는게 목적.(in 딥러닝)\n","궁극적으로 Y찾는것\n","이는, 긴 다항식의 답을 연립방정식으로 넣는 것과 유사하다.\n","\n","LDA(W가 해가 딱 위에 있음. 사람을 구별하기 위해 다르게함)\n","1이란 군집, 2란 군집 지들끼리 표준편차는 최소화, 단 평균차는 클수록 구별 쉬움\n","PCA(3차원에서 2차원으로 줄이면, 그림자. 해 위치 W.) 각 벡터를 보며, 가장 큰 벡터 위주\n","\n","오버피팅 해결법\n","1) 오토인코딩 : 중요한 것 위주로 사이즈를 줄이고, 디코딩 시엔 다시 복원\n","2) 드랍아웃: 값 소팅해서, 가장 작은 값들 10%를 0으로 만듬.\n","\n","\n","\"\"\""],"execution_count":null,"outputs":[]}]}