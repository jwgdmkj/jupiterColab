{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SoftmaxTF.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPQU+WO4RRTzHP19QRSGhjZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"ZcrBpr0is8Y6"},"source":["import numpy as np\n","import tensorflow as tf\n","\n","#Data\n","x_train = np.array([[1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5], [1,2,5,6],\n","                   [1,6,6,6], [1,7,7,7]], dtype = np.float32)\n","y_train = np.array([[0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0], \n","                   [1,0,0], [1,0,0]], dtype = np.float32)\n","W = tf.Variable(tf.random.normal([4,3]))\n","b = tf.Variable(tf.random.normal([3]))\n","\n","#H(x) 값을 출력함. x_data 행렬과 W행렬을 곱해서 하나의 값을 뽑아냄\n","def model_SoftmaxClassificationLC(x):\n","  return tf.matmul(x, W) + b\n","\n","# (데이터 - 정답)을 제곱(square)하고, 평균값내기(reduce_mean)\n","def cost_SoftmaxClassificationLC(model_x) :\n","  return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = model_x,\n","                                                                labels = y_train))\n","\n","# SGD: 경사하향법 하는 함수, 인자(learning rate)는 gradient양\n","def train_optimization(x) :\n","  with tf.GradientTape() as g:\n","    model_LC = model_SoftmaxClassificationLC(x) #x의 3*2와 랜덤(w) 2*1 행렬 곱해서, 3*1짜리 생성\n","    cost = cost_SoftmaxClassificationLC(model_LC)\n","  gradients = g.gradient(cost, [W, b]) #나온 오차(cost)에 [W,b]를 보내, 미분+기울기 구함\n","\n","    #오차역전파 구해, weight을 재정의함(SGD - Learning rate)\n","    #zip(gradinets, [W,b])를 통해, 다시 정의할 필요 없이 바로 다시 둘을 사용 가능\n","  tf.optimizers.SGD(0.01).apply_gradients(zip(gradients, [W,b]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t3b7AvNDvHgX","executionInfo":{"elapsed":11365,"status":"ok","timestamp":1638258020345,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"},"user_tz":-540},"outputId":"c7f7d4b7-353a-4e0a-ff54-cb2d50737973"},"source":["for step in range(2001) :\n","  train_optimization(x_train)\n","  if step%100 == 0:\n","    pred = model_SoftmaxClassificationLC(x_train)\n","    loss = cost_SoftmaxClassificationLC(pred)\n","    correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y_train, 1))\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","    print(\"Step: {}, \\t Accuracy: {}, \\t Loss:{}\".format(step, accuracy.numpy().flatten(),\n","                                                         loss))\n","\n","print('='*30)\n","x_test = np.array([[1,8,8,8]], dtype = np.float32)\n","model_test = tf.nn.softmax(model_SoftmaxClassificationLC(x_test))\n","print('Model Ttest')\n","print(\"model for [1,8,8,8]: \", tf.argmax(model_test, 1).numpy())\n","print('=' * 30)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Step: 0, \t Accuracy: [0.375], \t Loss:13.390893936157227\n","Step: 100, \t Accuracy: [0.375], \t Loss:1.115812063217163\n","Step: 200, \t Accuracy: [0.375], \t Loss:0.9250802993774414\n","Step: 300, \t Accuracy: [0.5], \t Loss:0.7996666431427002\n","Step: 400, \t Accuracy: [0.625], \t Loss:0.719741940498352\n","Step: 500, \t Accuracy: [0.875], \t Loss:0.6675014495849609\n","Step: 600, \t Accuracy: [0.875], \t Loss:0.6312598586082458\n","Step: 700, \t Accuracy: [0.875], \t Loss:0.6044679880142212\n","Step: 800, \t Accuracy: [0.875], \t Loss:0.5835670232772827\n","Step: 900, \t Accuracy: [0.875], \t Loss:0.5665581822395325\n","Step: 1000, \t Accuracy: [0.875], \t Loss:0.5522572994232178\n","Step: 1100, \t Accuracy: [0.875], \t Loss:0.539922833442688\n","Step: 1200, \t Accuracy: [0.875], \t Loss:0.5290670990943909\n","Step: 1300, \t Accuracy: [0.875], \t Loss:0.5193561911582947\n","Step: 1400, \t Accuracy: [0.875], \t Loss:0.5105538368225098\n","Step: 1500, \t Accuracy: [0.875], \t Loss:0.5024868249893188\n","Step: 1600, \t Accuracy: [0.875], \t Loss:0.4950265884399414\n","Step: 1700, \t Accuracy: [0.875], \t Loss:0.48807406425476074\n","Step: 1800, \t Accuracy: [0.875], \t Loss:0.48155251145362854\n","Step: 1900, \t Accuracy: [0.875], \t Loss:0.4754011929035187\n","Step: 2000, \t Accuracy: [0.875], \t Loss:0.46957099437713623\n","==============================\n","Model Ttest\n","model for [1,8,8,8]:  [0]\n","==============================\n"]}]},{"cell_type":"code","metadata":{"id":"kFZ0tz4Hyvna"},"source":["import tensorflow as tf\n","import numpy as np\n","import torch.nn as nn\n","from tensorflow.keras.datasets import mnist\n","\n","batch_size = 128 #60000개를 한번에 행렬로 가져올 수 없으니\n","#128개씩 나눠가져옴\n","\n","#사용할 중간 레이어들. 256으로 되어있다.\n","#256으로 되어있을땐 확률이 91%로 나온다.\n","\n","#만일 히든레이어를 돌리지 않을 경우, 확률은 86%로 떨어지는 것을 알 수 있다.(softmax)\n","#또한, 히든레이어 노드개수를 1로 설정할 경우엔, 10%대로 대폭 감소한다.\n","nH1=256\n","nH2=256\n","nH3=256\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train.astype('float32'), x_test.astype('float32')\n","#데이터를 [총개수/784, 784]로 변경 \n","x_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])\n","x_train, x_test = x_train/255., x_test/255. #0~1사이로 변경\n","y_train, y_test = tf.one_hot(y_train, depth=10), tf.one_hot(y_test, depth=10)\n","\n","#numpy array나 list를 tensor dataset으로 변환\n","#이후, 데이터셋을 임의로 배치사이즈만큼 섞음\n","train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","train_data = train_data.repeat().shuffle(5000).batch(batch_size)\n","\n","#히든레이어 적용\n","#W는 가중치넣는 배열, b는 bias개수\n","class ANN(nn.Module) :\n","  def __init__(self):\n","    self.W_1 = tf.Variable(tf.random.normal(shape=[784, nH1]))\n","    self.W_2 = tf.Variable(tf.random.normal(shape=[nH1, nH2]))\n","    self.W_3 = tf.Variable(tf.random.normal(shape=[nH2, nH3]))\n","    self.W_Out = tf.Variable(tf.random.normal(shape=[nH3, 10]))\n","    #self.W_Out = tf.Variable(tf.random.normal(shape=[784, 10]))\n","    self.b_1 = tf.Variable(tf.random.normal(shape=[nH1]))\n","    self.b_2 = tf.Variable(tf.random.normal(shape=[nH1]))\n","    self.b_3 = tf.Variable(tf.random.normal(shape=[nH1]))\n","    self.b_Out = tf.Variable(tf.random.normal(shape=[10]))\n","    #self.b_Out = tf.Variable(tf.random.normal(shape=[10]))\n","\n","  def __call__(self, x) :\n","    H1_Out = tf.nn.relu(tf.matmul(x, self.W_1) + self.b_1)\n","    H2_Out = tf.nn.relu(tf.matmul(H1_Out, self.W_2) + self.b_2)\n","    H3_Out = tf.nn.relu(tf.matmul(H2_Out, self.W_3) + self.b_3)\n","    Out = tf.matmul(H3_Out, self.W_Out) + self.b_Out\n","    #Out = tf.matmul(x, self.W_Out) + self.b_Out\n","    return Out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GulPRVgc0KsM"},"source":["W_12 = tf.Variable(tf.random.normal(shape=[784, nH1]))\n","W_22 = tf.Variable(tf.random.normal(shape=[nH1, nH2]))\n","W_32 = tf.Variable(tf.random.normal(shape=[nH2, nH3]))\n","W_Out2 = tf.Variable(tf.random.normal(shape=[nH3, 10]))\n","    #self.W_Out = tf.Variable(tf.random.normal(shape=[784, 10]))\n","b_12 = tf.Variable(tf.random.normal(shape=[nH1]))\n","b_22 = tf.Variable(tf.random.normal(shape=[nH1]))\n","b_32 = tf.Variable(tf.random.normal(shape=[nH1]))\n","b_Out2 = tf.Variable(tf.random.normal(shape=[10]))\n","\n","\n","def caller(x) :\n","  H1_Out = tf.nn.relu(tf.matmul(x, W_12) + b_12)\n","  H2_Out = tf.nn.relu(tf.matmul(H1_Out, W_22) + b_22)\n","  H3_Out = tf.nn.relu(tf.matmul(H2_Out, W_32) + b_32)\n","  Out = tf.matmul(H3_Out, W_Out2) + b_Out2\n","  #Out = tf.matmul(x, self.W_Out) + self.b_Out\n","  return Out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"XUO4fbIDyvpt","outputId":"4000604f-91ba-44a5-ce9a-44d3c2842858"},"source":["\n","\\ANN_model = ANN() #__init__ 실행\n","optimizer = tf.optimizers.Adam(0.01)\n","\n","#softmax 비용함수(비용 구하기) 상세한건 이전거 참조\n","def cost_ANN_mnist(x,y) :\n","  y = tf.cast(y, tf.int64)\n","  loss = tf.nn.softmax_cross_entropy_with_logits(logits=x, labels=y)\n","  return tf.reduce_mean(loss)\n","\n","#tf.argmax 비교 통해, 맞는지 확인\n","def accuracy(x,y) :\n","  correct = tf.equal(tf.argmax(x,1), tf.argmax(y,1))\n","  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","  return accuracy\n","\n","def train_optimization(x,y) :\n","  with tf.GradientTape() as g:\n","    pred = caller(x) #Ann 모델에 x 데이터 삽입\n","    cost = cost_ANN_mnist(pred, y) #비용 구하기\n","  gradients = g.gradient(cost,[W_12,\n","                                             b_Out2])\n","  #기존 gradients와 다르게 뒤에 unconnected...를 정의함으로써, 연결되지않는 그래디언트로 인한 \n","  #오류를 제거\n","  optimizer.apply_gradients(zip(gradients, [W_12\n","                                            , b_Out2]))\n","\n","#300개씩 가져와 batch_x, batch_y에 넣음\n","#섞인 train data들을, 300개(+y는 1개씩)가져와 batch_x, y에 넣음\n","for step, (batch_x, batch_y) in enumerate(train_data.take(300), 1):\n","  train_optimization(batch_x, batch_y)\n","  #batch_x 넣고, 러닝 돌린다. 이를 batch_y와 비교해, 손실 구하고\n","  #정확도 역시 판단.\n","  if step%10 == 0:\n","    pred = caller(batch_x) #__call__ 실행\n","    loss = cost_ANN_mnist(pred, batch_y)\n","    acc = accuracy(pred, batch_y)\n","    print(\"Step: {}, \\t Accuracy: {}, \\t Loss:{}\".format(step, acc.numpy().flatten(),\n","                                                         loss.numpy().flatten()))\n","\n","print('='*30)\n","print('Model Ttest')\n","print(\"Test accuracy: \", accuracy(caller(x_test), y_test).numpy().flatten())\n","print('=' * 30)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Step: 10, \t Accuracy: [0.9296875], \t Loss:[210.1358]\n","Step: 20, \t Accuracy: [0.9296875], \t Loss:[184.19086]\n","Step: 30, \t Accuracy: [0.9140625], \t Loss:[304.43677]\n","Step: 40, \t Accuracy: [0.96875], \t Loss:[36.94867]\n","Step: 50, \t Accuracy: [0.9296875], \t Loss:[228.90591]\n","Step: 60, \t Accuracy: [0.9375], \t Loss:[154.9779]\n","Step: 70, \t Accuracy: [0.921875], \t Loss:[273.5207]\n","Step: 80, \t Accuracy: [0.9140625], \t Loss:[195.20493]\n","Step: 90, \t Accuracy: [0.921875], \t Loss:[144.4783]\n","Step: 100, \t Accuracy: [0.9609375], \t Loss:[99.49069]\n","Step: 110, \t Accuracy: [0.921875], \t Loss:[210.35207]\n","Step: 120, \t Accuracy: [0.953125], \t Loss:[95.05138]\n","Step: 130, \t Accuracy: [0.96875], \t Loss:[48.155903]\n","Step: 140, \t Accuracy: [0.9296875], \t Loss:[124.88468]\n","Step: 150, \t Accuracy: [0.921875], \t Loss:[157.46167]\n","Step: 160, \t Accuracy: [0.921875], \t Loss:[158.962]\n","Step: 170, \t Accuracy: [0.921875], \t Loss:[209.60706]\n","Step: 180, \t Accuracy: [0.9609375], \t Loss:[43.99639]\n","Step: 190, \t Accuracy: [0.90625], \t Loss:[139.24452]\n","Step: 200, \t Accuracy: [0.953125], \t Loss:[118.247116]\n","Step: 210, \t Accuracy: [0.9375], \t Loss:[126.176414]\n","Step: 220, \t Accuracy: [0.953125], \t Loss:[59.443977]\n","Step: 230, \t Accuracy: [0.953125], \t Loss:[94.15585]\n","Step: 240, \t Accuracy: [0.9609375], \t Loss:[200.31238]\n","Step: 250, \t Accuracy: [0.9296875], \t Loss:[79.746735]\n","Step: 260, \t Accuracy: [0.9375], \t Loss:[103.58037]\n","Step: 270, \t Accuracy: [0.96875], \t Loss:[137.86949]\n","Step: 280, \t Accuracy: [0.9765625], \t Loss:[66.42619]\n","Step: 290, \t Accuracy: [0.9609375], \t Loss:[72.18809]\n","Step: 300, \t Accuracy: [0.953125], \t Loss:[156.45596]\n","==============================\n","Model Ttest\n","Test accuracy:  [0.9282]\n","==============================\n"]}]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"FYj4wxGmHB1S","executionInfo":{"elapsed":919,"status":"ok","timestamp":1638264532106,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"},"user_tz":-540},"outputId":"fda1b9d7-39b2-48e0-c517-edd095ebf4a8"},"source":["ran1 = tf.Variable(tf.random.normal(shape = [10]))\n","ran2 = tf.Variable(tf.random.normal(shape = [1,10]))\n","\n","ran1"],"execution_count":null,"outputs":[{"data":{"text/plain":["<tf.Variable 'Variable:0' shape=(10,) dtype=float32, numpy=\n","array([-0.18430068, -1.7281617 , -0.8465648 , -0.33136225, -0.16282019,\n","        0.6402151 ,  0.06544773,  0.7763703 , -0.12220418,  0.16584687],\n","      dtype=float32)>"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"BknLYPqQHN6T","executionInfo":{"elapsed":11,"status":"ok","timestamp":1638264534340,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"},"user_tz":-540},"outputId":"6ff103ae-c8b0-4c0b-deb7-449d368cde02"},"source":["ran2"],"execution_count":null,"outputs":[{"data":{"text/plain":["<tf.Variable 'Variable:0' shape=(1, 10) dtype=float32, numpy=\n","array([[ 0.25651702,  0.3352808 , -0.524567  ,  0.12463368, -1.1703341 ,\n","         0.42329538,  0.13766249,  0.73083544,  0.6887514 ,  0.2929625 ]],\n","      dtype=float32)>"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}]}]}