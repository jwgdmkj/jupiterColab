{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MnistTfKrs_1202.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMFRCCOj8shgL2y/GZlArJB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KcY37sldw8mz","executionInfo":{"status":"ok","timestamp":1638410289493,"user_tz":-540,"elapsed":49282,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}},"outputId":"02a249e2-c615-4080-eac8-332fa3cb61eb"},"source":["# -*- coding: utf-8 -*-\n","\"\"\"tf25_mnist_ann_noclass.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1W2N7kxyOrofSJ045SjXIlUnyeZ5SXRnk\n","\"\"\"\n","\n","import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras.datasets import mnist\n","\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","train_images = train_images.astype('float32').reshape([-1,784]) / 255.\n","test_images = test_images.astype('float32').reshape([-1,784]) / 255.\n","train_labels = tf.one_hot(train_labels, depth=10)\n","test_labels = tf.one_hot(test_labels, depth=10)\n","batch_size = 1000\n","\"\"\"\n","tf.data.Dataset.from_tensor_slices 함수는 tf.data.Dataset 를 생성하는 함수로 \n","입력된 텐서로부터 slices를 생성합니다. 예를 들어 MNIST의 학습데이터 (60000, 28, 28)가\n","되면, 60000개의 slices로 만들고 각각의 slice는 28×28의 이미지 크기를 갖게 됩니다.\n","\"\"\"\n","train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n","train_data = train_data.repeat().shuffle(5000).batch(batch_size)\n","\n","nH1 = 256\n","nH2 = 256\n","nH3 = 256\n","W_1 = tf.Variable(tf.random.normal([784,nH1]))\n","b_1 = tf.Variable(tf.random.normal([nH1]))\n","W_2 = tf.Variable(tf.random.normal([nH1,nH2]))\n","b_2 = tf.Variable(tf.random.normal([nH2]))\n","W_3 = tf.Variable(tf.random.normal([nH2,nH3]))\n","b_3 = tf.Variable(tf.random.normal([nH3]))\n","W_o = tf.Variable(tf.random.normal([nH3,10]))\n","b_o = tf.Variable(tf.random.normal([10]))\n","\n","def model_SoftmaxNN_LC(x):\n","  H_1 = tf.nn.relu(tf.matmul(x,W_1)+b_1)\n","  H_2 = tf.nn.relu(tf.matmul(H_1,W_2)+b_2)\n","  H_3 = tf.nn.relu(tf.matmul(H_2,W_3)+b_3)\n","  Out = tf.matmul(H_3,W_o)+b_o\n","  return Out\n","\n","def cost_Softmax(x,y):\n","  return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=x, labels=y))\n","\n","def train_optimization(MNIST_images,MNIST_labels):\n","  with tf.GradientTape() as g:\n","    model_AI = model_SoftmaxNN_LC(MNIST_images)\n","    cost = cost_Softmax(model_AI, MNIST_labels)\n","  gradients = g.gradient(cost, [W_1,W_2,W_3,W_o,b_1,b_2,b_3,b_o])\n","  tf.optimizers.Adam(0.01).apply_gradients(zip(gradients, [W_1,W_2,W_3,W_o,b_1,b_2,b_3,b_o]))\n","\n","for epoch in range(10):\n","  for step, (batch_images, batch_labels) in enumerate(train_data.take(60),1):\n","    train_optimization(batch_images, batch_labels)\n","    if step % 10 == 0:\n","      pred = tf.nn.softmax(model_SoftmaxNN_LC(batch_images))\n","      accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred,1),tf.argmax(batch_labels,1)), tf.float32))\n","      print('Epoch: {} \\t Step: {} \\t Accuracy: {}'.format(epoch,step,accuracy.numpy()))\n","\n","print('-'*30)\n","test_model = tf.nn.softmax(model_SoftmaxNN_LC(test_images))\n","test_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(test_model,1),tf.argmax(test_labels,1)), tf.float32))\n","print('Test Accuracy: ', test_accuracy.numpy())"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0 \t Step: 10 \t Accuracy: 0.597000002861023\n","Epoch: 0 \t Step: 20 \t Accuracy: 0.7409999966621399\n","Epoch: 0 \t Step: 30 \t Accuracy: 0.7889999747276306\n","Epoch: 0 \t Step: 40 \t Accuracy: 0.8270000219345093\n","Epoch: 0 \t Step: 50 \t Accuracy: 0.8690000176429749\n","Epoch: 0 \t Step: 60 \t Accuracy: 0.8980000019073486\n","Epoch: 1 \t Step: 10 \t Accuracy: 0.8769999742507935\n","Epoch: 1 \t Step: 20 \t Accuracy: 0.9210000038146973\n","Epoch: 1 \t Step: 30 \t Accuracy: 0.9259999990463257\n","Epoch: 1 \t Step: 40 \t Accuracy: 0.9380000233650208\n","Epoch: 1 \t Step: 50 \t Accuracy: 0.9359999895095825\n","Epoch: 1 \t Step: 60 \t Accuracy: 0.9419999718666077\n","Epoch: 2 \t Step: 10 \t Accuracy: 0.9430000185966492\n","Epoch: 2 \t Step: 20 \t Accuracy: 0.9509999752044678\n","Epoch: 2 \t Step: 30 \t Accuracy: 0.9330000281333923\n","Epoch: 2 \t Step: 40 \t Accuracy: 0.9409999847412109\n","Epoch: 2 \t Step: 50 \t Accuracy: 0.972000002861023\n","Epoch: 2 \t Step: 60 \t Accuracy: 0.968999981880188\n","Epoch: 3 \t Step: 10 \t Accuracy: 0.9549999833106995\n","Epoch: 3 \t Step: 20 \t Accuracy: 0.9629999995231628\n","Epoch: 3 \t Step: 30 \t Accuracy: 0.9700000286102295\n","Epoch: 3 \t Step: 40 \t Accuracy: 0.9760000109672546\n","Epoch: 3 \t Step: 50 \t Accuracy: 0.9700000286102295\n","Epoch: 3 \t Step: 60 \t Accuracy: 0.9760000109672546\n","Epoch: 4 \t Step: 10 \t Accuracy: 0.9639999866485596\n","Epoch: 4 \t Step: 20 \t Accuracy: 0.9789999723434448\n","Epoch: 4 \t Step: 30 \t Accuracy: 0.9750000238418579\n","Epoch: 4 \t Step: 40 \t Accuracy: 0.9710000157356262\n","Epoch: 4 \t Step: 50 \t Accuracy: 0.9800000190734863\n","Epoch: 4 \t Step: 60 \t Accuracy: 0.972000002861023\n","Epoch: 5 \t Step: 10 \t Accuracy: 0.968999981880188\n","Epoch: 5 \t Step: 20 \t Accuracy: 0.9760000109672546\n","Epoch: 5 \t Step: 30 \t Accuracy: 0.9829999804496765\n","Epoch: 5 \t Step: 40 \t Accuracy: 0.9829999804496765\n","Epoch: 5 \t Step: 50 \t Accuracy: 0.9829999804496765\n","Epoch: 5 \t Step: 60 \t Accuracy: 0.9829999804496765\n","Epoch: 6 \t Step: 10 \t Accuracy: 0.9819999933242798\n","Epoch: 6 \t Step: 20 \t Accuracy: 0.9879999756813049\n","Epoch: 6 \t Step: 30 \t Accuracy: 0.9789999723434448\n","Epoch: 6 \t Step: 40 \t Accuracy: 0.9829999804496765\n","Epoch: 6 \t Step: 50 \t Accuracy: 0.9850000143051147\n","Epoch: 6 \t Step: 60 \t Accuracy: 0.984000027179718\n","Epoch: 7 \t Step: 10 \t Accuracy: 0.9869999885559082\n","Epoch: 7 \t Step: 20 \t Accuracy: 0.9909999966621399\n","Epoch: 7 \t Step: 30 \t Accuracy: 0.9900000095367432\n","Epoch: 7 \t Step: 40 \t Accuracy: 0.9900000095367432\n","Epoch: 7 \t Step: 50 \t Accuracy: 0.9890000224113464\n","Epoch: 7 \t Step: 60 \t Accuracy: 0.9890000224113464\n","Epoch: 8 \t Step: 10 \t Accuracy: 0.9869999885559082\n","Epoch: 8 \t Step: 20 \t Accuracy: 0.9919999837875366\n","Epoch: 8 \t Step: 30 \t Accuracy: 0.9869999885559082\n","Epoch: 8 \t Step: 40 \t Accuracy: 0.9919999837875366\n","Epoch: 8 \t Step: 50 \t Accuracy: 0.9890000224113464\n","Epoch: 8 \t Step: 60 \t Accuracy: 0.9900000095367432\n","Epoch: 9 \t Step: 10 \t Accuracy: 0.9850000143051147\n","Epoch: 9 \t Step: 20 \t Accuracy: 0.9909999966621399\n","Epoch: 9 \t Step: 30 \t Accuracy: 0.984000027179718\n","Epoch: 9 \t Step: 40 \t Accuracy: 0.9950000047683716\n","Epoch: 9 \t Step: 50 \t Accuracy: 0.9900000095367432\n","Epoch: 9 \t Step: 60 \t Accuracy: 0.9959999918937683\n","------------------------------\n","Test Accuracy:  0.956\n"]}]},{"cell_type":"code","metadata":{"id":"SXdeE4-xw_37","executionInfo":{"status":"ok","timestamp":1638412672734,"user_tz":-540,"elapsed":271,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}}},"source":["#made with PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","\n","train_images = datasets.MNIST(root = '../Data', train = True, download = True,\n","                              transform = transforms.ToTensor())\n","train_data = datasets.MNIST(root = '../Data', train = True, download = True,\n","                              transform = transforms.ToTensor())\n","#라벨 붙여줄 것들\n","train_labels = torch.zeros(len(train_labels), 10)\n","test_labels = torch.zeros(len(test_labels), 10)\n","\n","batch_size = 1000\n","#train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n","#train_data = train_data.repeat().shuffle(5000).batch(batch_size)\n","\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('dataset/',train = True,download = True,\n","                 transform = transforms.Compose([\n","                     transforms.ToTensor(),\n","                     transforms.Normalize(mean = (0.5,), std = (0.5,))\n","                 ])),\n","    batch_size = batch_size,\n","    shuffle = True)\n","\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('dataset',train=False,\n","                  transform = transforms.Compose([\n","                      transforms.ToTensor(),\n","                      transforms.Normalize((0.5,), (0,5))\n","                  ])),\n","    batch_size = batch_size,\n","    shuffle = True)\n","\n","\n","nH1 = 256\n","nH2 = 256\n","nH3 = 256\n","W_1 = torch.randn([784,nH1], requires_grad = True)\n","W_2 = torch.randn([nH1,nH2], requires_grad = True)\n","W_3 = torch.randn([nH2,nH3], requires_grad = True)\n","W_o = torch.randn([nH3,10], requires_grad = True)\n","b_1 = torch.randn([nH1], requires_grad = True)\n","b_2 = torch.randn([nH2], requires_grad = True)\n","b_3 = torch.randn([nH3], requires_grad = True)\n","b_o = torch.randn([10], requires_grad = True)\n","optimizer = torch.optim.SGD([W_1, W_2, W_3, W_o, b_1, b_2, b_3, b_o], lr = 0.01)\n","\n","def model_SoftmaxNN_LC(x):\n","  H_1 = F.softmax(torch.matmul(x,W_1)+b_1)\n","  H_2 = F.softmax(torch.matmul(H_1,W_2)+b_2)\n","  H_3 = F.softmax(torch.matmul(H_2,W_3)+b_3)\n","  Out = torch.matmul(H_3,W_o)+b_o\n","  return Out\n","\n","#def cost_Softmax(x,y):\n","  #return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=x, labels=y))\n","\n","def train_optimization(MNIST_images,MNIST_labels):\n","  with tf.GradientTape() as g:\n","    model_AI = model_SoftmaxNN_LC(MNIST_images)\n","    cost = F.cross_entropy(model_AI, MNIST_labels)\n","  gradients = g.gradient(cost, [W_1,W_2,W_3,W_o,b_1,b_2,b_3,b_o])\n","  tf.optimizers.Adam(0.01).apply_gradients(zip(gradients, [W_1,W_2,W_3,W_o,b_1,b_2,b_3,b_o]))"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"KHhByDpWGaD2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"JJt_x0fczS4j","executionInfo":{"status":"error","timestamp":1638412694857,"user_tz":-540,"elapsed":281,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}},"outputId":"69bae834-b49d-423b-ef8b-19ab80657e3d"},"source":["for epoch in range(10):\n","  for step, (batch_images, batch_labels) in enumerate(train_loader.take(60),1):\n","    train_optimization(batch_images, batch_labels)\n","    if step % 10 == 0:\n","      pred = tf.nn.softmax(model_SoftmaxNN_LC(batch_images))\n","      accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred,1),tf.argmax(batch_labels,1)), tf.float32))\n","      print('Epoch: {} \\t Step: {} \\t Accuracy: {}'.format(epoch,step,accuracy.numpy()))\n","\n","print('-'*30)\n","test_model = tf.nn.softmax(model_SoftmaxNN_LC(test_images))\n","test_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(test_model,1),tf.argmax(test_labels,1)), tf.float32))\n","print('Test Accuracy: ', test_accuracy.numpy())"],"execution_count":38,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-733e0f4ee81c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_SoftmaxNN_LC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'take'"]}]}]}