{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TF_BinaryClassfiy.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOr+e/o8owECzZz2dDBBeTD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"ohijU3Wj6O_I","executionInfo":{"status":"ok","timestamp":1637220391310,"user_tz":-540,"elapsed":315,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}}},"source":["import tensorflow as tf\n","import numpy as np\n","\n","x_data = np.array([[1,2], [2,3], [3,4], [4,4], [5,3], [6,2]], dtype = np.float32)\n","y_data = np.array([[0], [0], [0], [1], [1], [1]], dtype = np.float32)\n","W = tf.Variable(tf.random.normal([2,1])) #정규분포에서, 주어진 형태의 난수를 생성.\n","b = tf.Variable(tf.random.normal(([1]))) #즉, W는 [2,1]짜리 배열, b는 [1]짜리 데이터"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"LavxWlAh6jhY","executionInfo":{"status":"ok","timestamp":1637220391731,"user_tz":-540,"elapsed":11,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}}},"source":["# 행렬을 곱한 다음, 시그모이드를 취함\n","def model_BinaryClassification(x):\n","  return tf.sigmoid(tf.matmul(x, W) + b)\n","\n","# (데이터 - 정답)을 제곱(square)하고, 평균값내기(reduce_mean)\n","def cost_BinaryClassification(model_x) :\n","\n","  #1.-y_data = y_data에 -를 취하고, 1을 더한다.\n","  #log(1.-model_x) : model_x에 -를 취하고, 1을 더한 다음 log 취한다\n","  return tf.reduce_mean((-1) * y_data * tf.math.log(model_x) + \n","                        (-1) * (1.-y_data) * tf.math.log(1.-model_x))\n","\n","# SGD: 경사하향법 하는 함수, 인자(learning rate)는 gradient양\n","def train_optimization(x) :\n","  with tf.GradientTape() as g:\n","    model = model_BinaryClassification(x) #3*2와 2*1 행렬 곱해서, 3*1짜리 생성\n","    cost = cost_BinaryClassification(model)\n","    gradients = g.gradient(cost, [W, b]) #나온 오차(cost)에 [W,b]를 보내, 미분+기울기 구함\n","    \n","    #오차역전파 구해, weight을 재정의함(SGD - Learning rate)\n","    tf.optimizers.SGD(0.01).apply_gradients(zip(gradients, [W,b]))"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KjDhqkM66joJ","executionInfo":{"status":"ok","timestamp":1637220411487,"user_tz":-540,"elapsed":11088,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}},"outputId":"234154e8-7633-492b-be84-568c1bdd6464"},"source":["for step in range(2001) :\n","  train_optimization(x_data)\n","\n","  if step%100 == 0:\n","    pred = model_BinaryClassification(x_data)\n","    loss = cost_BinaryClassification(pred)\n","    #계산을 해서, 참이면 True, 거짓이면 False로 텐서를 캐스팅\n","    prediction = tf.cast(pred > 0.5, dtype = tf.float32)\n","    #prediction과 y_data가 일치한다면 1, 아님 0을 모은 다음 mean처리함\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, y_data), dtype = tf.float32))\n","    print(\"step : {}, \\t Accuracy: {}, \\t Loss: {}\".format(step, accuracy.numpy().flatten(), loss))"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["step : 0, \t Accuracy: [0.16666667], \t Loss: 1.4331293106079102\n","step : 100, \t Accuracy: [0.33333334], \t Loss: 0.8562169671058655\n","step : 200, \t Accuracy: [0.6666667], \t Loss: 0.5765575766563416\n","step : 300, \t Accuracy: [0.8333333], \t Loss: 0.4414792060852051\n","step : 400, \t Accuracy: [1.], \t Loss: 0.3683284819126129\n","step : 500, \t Accuracy: [1.], \t Loss: 0.3229929506778717\n","step : 600, \t Accuracy: [1.], \t Loss: 0.29179438948631287\n","step : 700, \t Accuracy: [1.], \t Loss: 0.2686583995819092\n","step : 800, \t Accuracy: [1.], \t Loss: 0.25056031346321106\n","step : 900, \t Accuracy: [1.], \t Loss: 0.23583924770355225\n","step : 1000, \t Accuracy: [1.], \t Loss: 0.22350859642028809\n","step : 1100, \t Accuracy: [1.], \t Loss: 0.2129439115524292\n","step : 1200, \t Accuracy: [1.], \t Loss: 0.2037295699119568\n","step : 1300, \t Accuracy: [1.], \t Loss: 0.19557708501815796\n","step : 1400, \t Accuracy: [1.], \t Loss: 0.18827934563159943\n","step : 1500, \t Accuracy: [1.], \t Loss: 0.18168319761753082\n","step : 1600, \t Accuracy: [1.], \t Loss: 0.17567260563373566\n","step : 1700, \t Accuracy: [1.], \t Loss: 0.17015786468982697\n","step : 1800, \t Accuracy: [1.], \t Loss: 0.16506806015968323\n","step : 1900, \t Accuracy: [1.], \t Loss: 0.16034652292728424\n","step : 2000, \t Accuracy: [1.], \t Loss: 0.15594713389873505\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S6O6usho6jvG","executionInfo":{"status":"ok","timestamp":1637222821458,"user_tz":-540,"elapsed":268,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}},"outputId":"82941ddf-e91d-4d89-a76a-f9ce6eb8a11c"},"source":["print('='*100)\n","#x_test는, 4*4짜리 행렬데이터.\n","x_test = np.array([[6,1]], dtype = np.float32)\n","model_test = model_BinaryClassification(x_test)\n","print(\"model for [6,1]: \", model_test.numpy()) #numpy()함수 통해, 출력에 적합하게 캐스팅\n","print('=' * 100)"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["====================================================================================================\n","model for [6,1]:  [[0.99975264]]\n","====================================================================================================\n","[[-1.]\n"," [-1.]\n"," [-1.]\n"," [-2.]\n"," [-2.]\n"," [-2.]]\n"]}]}]}