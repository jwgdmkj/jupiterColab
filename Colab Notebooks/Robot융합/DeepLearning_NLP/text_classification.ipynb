{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_classification.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM0qi1jFTzxsg8Bs5GC0W2F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oh7zmliFGFyH","executionInfo":{"status":"ok","timestamp":1637827603100,"user_tz":-540,"elapsed":374,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}},"outputId":"55257bd0-975f-4ca4-d819-e1f409fb9f3b"},"source":["import os\n","current_path = os.getcwd()\n","print(current_path)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["/root\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"1tn4fzmtIqYt"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7kjaPagWGgut","executionInfo":{"status":"ok","timestamp":1637827731332,"user_tz":-540,"elapsed":5224,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}}},"source":["import zipfile\n","DATA_IN_PATH = '/content/drive/MyDrive/Colab Notebooks/Robot융합/DeepLearning_NLP/data_in/'\n","file_list = ['labeledTrainData.tsv.zip', 'unlabeledTrainData.tsv.zip', 'testData.tsv.zip']\n","\n","for file in file_list:\n","  zipRef = zipfile.ZipFile(DATA_IN_PATH + file, 'r')\n","  zipRef.extractall(DATA_IN_PATH)\n","  zipRef.close()"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kZ6yFc_NJBdV","executionInfo":{"status":"ok","timestamp":1637827888468,"user_tz":-540,"elapsed":1308,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}},"outputId":"363dac56-0851-496c-f443-da8c85c7d421"},"source":["import numpy as np\n","import pandas as pd\n","import os\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","print(\"File size: \")\n","\n","for file in os.listdir(DATA_IN_PATH) :\n","  if 'tsv' in file and 'zip' not in file:\n","    print(file.ljust(30) + str(round(os.path.getsize(DATA_IN_PATH + file)/1000000, 2)) + 'MB')\n","\n","train_data = pd.read_csv(DATA_IN_PATH + 'labeledTrainData.tsv', header = 0, delimiter = '\\t', \n","                         quoting = 3)\n","print('The # of entire training data: {}'.format(len(train_data)))\n","\n","train_length = train_data['review'].apply(len)\n","train_length.head()"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["File size: \n","labeledTrainData.tsv          33.56MB\n","unlabeledTrainData.tsv        67.28MB\n","testData.tsv                  32.72MB\n","The # of entire training data: 25000\n"]},{"output_type":"execute_result","data":{"text/plain":["0    2304\n","1     948\n","2    2451\n","3    2247\n","4    2233\n","Name: review, dtype: int64"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"Tz_EaV9uJl1X","executionInfo":{"status":"ok","timestamp":1637828009316,"user_tz":-540,"elapsed":1045,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}}},"source":["import re\n","import json\n","import pandas as pd\n","import numpy as np\n","import nltk\n","from bs4 import BeautifulSoup\n","from nltk.corpus import stopwords\n","from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.python.keras.preprocessing.text import Tokenizer\n","\n","DATA_IN_PATH = '/content/drive/MyDrive/Colab Notebooks/Robot융합/DeepLearning_NLP/data_in/'\n","train_data = pd.read_csv(DATA_IN_PATH + 'labeledTrainData.tsv', header = 0, delimiter = '\\t',\n","                         quoting = 3)"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YEd9CL0hJl7D","executionInfo":{"status":"ok","timestamp":1637828366283,"user_tz":-540,"elapsed":417,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}},"outputId":"fa36aa11-33ef-4aa2-dee4-0cbf1e46ef67"},"source":["review = train_data['review'][0] #첫번째 리뷰 가져와\n","review_text = BeautifulSoup(review, \"html5lib\").get_text() #html태그 제거\n","review_text = re.sub(\"[^a`zA-Z]\", \" \", review_text) #영어문자 제외한 나머진 전부 공백으로\n","\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english')) #영어 불용어 집합 구성\n","\n","review_text = review_text.lower()\n","words = review_text.split() # 소문자 변환 후, 단어마다 나눠 리스트로 만듬\n","words = [w for w in words if not w in stop_words] #불용어 제외 리스트 구성\n","print(words)\n","clean_reivew = ' '.join(words) #단어리스트를 다시 하나의 글로 합침\n","print(clean_reivew)"],"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","['w', 'mj', 'w', 'z', 'mj', 'v', 'ja', 'mj', 'mj', 'mj', 'c', 'j', 'p', 'w', 'mj', 'b', 'mj', 'na', 'j', 'p', 'mj', 'l', 'mj', 'ba', 'b', 'mj', 'mj', 'ja', 'w', 'h']\n","w mj w z mj v ja mj mj mj c j p w mj b mj na j p mj l mj ba b mj mj ja w h\n"]}]},{"cell_type":"code","metadata":{"id":"rn2OcbDgJmBj","executionInfo":{"status":"ok","timestamp":1637828598160,"user_tz":-540,"elapsed":32707,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}}},"source":["def preprocessing(reivew, remove_stopwords = False) :\n","  review_text = BeautifulSoup(review, \"html5lib\").get_text()\n","  review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n","  words = review_text.lower().split()\n","\n","  if remove_stopwords:\n","    stops = set(stopwords.words(\"english\"))\n","    words = [w for w in words if not w in stops]\n","    clean_review = ' '.join(words)\n","  else : #불용어 제거 안 하는 경우\n","    clean_review = ' '.join(words)\n","  return clean_review\n","\n","clean_train_reviews = []\n","for review in train_data['review']:\n","  clean_train_reviews.append(preprocessing(review, remove_stopwords = True))\n","\n","clean_train_df = pd.DataFrame({'review' : clean_train_reviews,\n","                               'sentiment' : train_data['sentiment']})"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pOJwuCPGMKen","executionInfo":{"status":"ok","timestamp":1637829372926,"user_tz":-540,"elapsed":6403,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}},"outputId":"4dec2d8a-38db-497b-f0a8-dbd2cbabfe0f"},"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(clean_train_reviews)\n","text_sequences = tokenizer.texts_to_sequences(clean_train_reviews)\n","\n","MAX_SEQUENCE_LENGTH = 174\n","train_inputs = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n","print('Train data: ', train_inputs.shape)\n","\n","train_labels = np.array(train_data['sentiment'])\n","print('Label: ', train_labels.shape)"],"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Train data:  (25000, 174)\n","Label:  (25000,)\n"]}]},{"cell_type":"code","metadata":{"id":"n7aYai-jMKlZ","executionInfo":{"status":"ok","timestamp":1637829378930,"user_tz":-540,"elapsed":821,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","\n","DATA_IN_PATH = '/content/drive/MyDrive/Colab Notebooks/Robot융합/DeepLearning_NLP/data_in/'\n","DATA_OUT_PATH = '/content/drive/MyDrive/Colab Notebooks/Robot융합/DeepLearning_NLP/data_in/'\n","TRAIN_CLEAN_DATA = 'train_clean.csv'\n","RANDOM_SEED = 42\n","TEST_SPLIT = 0.2"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":523},"id":"POLm_pChML_p","executionInfo":{"status":"error","timestamp":1637829381599,"user_tz":-540,"elapsed":422,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}},"outputId":"6d103031-e187-4ca4-e602-dc709cc8a819"},"source":["train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)\n","reviews = list(train_data['review'])\n","sentiments = list(train_data['sentiment'])\n","vectorizer = TfidfVectorizer(min_df = 0.0, analyzer = \"char\", sublinear_tf = True, \n","                             ngram_range = (1,3), max_features = 5000)\n","X= vectorizer.fit_transform(reviews)\n","y= np.array(sentiment)\n","\n","X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT,\n","                                                    random_state = RANDOM_SEED)\n","X_train.shape, y_train.shape, X_eval.shape, y_eval.shape\n","print(\"\")\n","lgs = LogisticRegression(class_weight = 'balanced')\n","lgs.fit(X_train, y_train)\n","predicted = lgs.predict(X_eval)\n","print(\"Accuracy: %f\" % lgs.score(X_eval, y_eval))"],"execution_count":58,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-58-a9fb9b05d337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_IN_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTRAIN_CLEAN_DATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m vectorizer = TfidfVectorizer(min_df = 0.0, analyzer = \"char\", sublinear_tf = True, \n\u001b[1;32m      5\u001b[0m                              ngram_range = (1,3), max_features = 5000)\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Robot융합/DeepLearning_NLP/data_in/train_clean.csv'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"Z6LANwjvMMGd","executionInfo":{"status":"error","timestamp":1637829336348,"user_tz":-540,"elapsed":552,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}},"outputId":"bcdb7f93-0cac-43dd-fae4-61f9439c16de"},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","X_train1 = X_train.toarray()\n","X_eval1 = X_eval.toarray()\n","\n","model = Sequential()\n","model.add(Dense(1, activation = 'sigmoid'))\n","model.compile(loss = 'binary_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])\n","model.fit(X_train1, y_train, epochs=200, verbose=1)\n","\n","_, accuracy = model.evaluate(X_eval1, y_eval)\n","print('Accuracy: ', accuracy)\n","model.summary()"],"execution_count":54,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-54-209110c6d78f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_eval1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"]}]}]}