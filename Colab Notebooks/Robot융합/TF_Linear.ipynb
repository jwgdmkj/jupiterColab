{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TF_Linear.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO9t9wgSN5aVBnKtKEpc7iK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"ZK3DfojcdvI2","executionInfo":{"status":"ok","timestamp":1637221729391,"user_tz":-540,"elapsed":2649,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}}},"source":["import tensorflow as tf\n","import numpy as np\n","\n","x_data = np.array([[1,1], [2,2], [3,3]], dtype = np.float32)\n","y_data = np.array([[10], [20], [30]], dtype = np.float32)\n","W = tf.Variable(tf.random.normal([2,1])) #정규분포에서, 주어진 형태의 난수를 생성.\n","b = tf.Variable(tf.random.normal(([1]))) #즉, W는 [2,1]짜리 배열, b는 [1]짜리 데이터"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"1l6k_USf94mo","executionInfo":{"status":"ok","timestamp":1637221729392,"user_tz":-540,"elapsed":6,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}}},"source":["#H(x) 값을 출력함. x_data 행렬과 W행렬을 곱해서 하나의 값을 뽑아냄\n","def model_LinearRegression(x):\n","  return tf.matmul(x, W) + b\n","\n","# (데이터 - 정답)을 제곱(square)하고, 평균값내기(reduce_mean)\n","def cost_LinearRegression(model_x) :\n","  return tf.reduce_mean(tf.square(model_x - y_data))\n","\n","# SGD: 경사하향법 하는 함수, 인자(learning rate)는 gradient양\n","def train_optimization(x) :\n","  with tf.GradientTape() as g:\n","    model = model_LinearRegression(x) #x의 3*2와 랜덤(w) 2*1 행렬 곱해서, 3*1짜리 생성\n","    cost = cost_LinearRegression(model)\n","    gradients = g.gradient(cost, [W, b]) #나온 오차(cost)에 [W,b]를 보내, 미분+기울기 구함\n","\n","    #오차역전파 구해, weight을 재정의함(SGD - Learning rate)\n","    #zip(gradinets, [W,b])를 통해, 다시 정의할 필요 없이 바로 다시 둘을 사용 가능\n","    tf.optimizers.SGD(0.01).apply_gradients(zip(gradients, [W,b]))"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"UHBKofZJxlio","executionInfo":{"status":"ok","timestamp":1637221737287,"user_tz":-540,"elapsed":7901,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}}},"source":["for step in range(2001) :\n","  train_optimization(x_data)"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iBooILkLxpph","executionInfo":{"status":"ok","timestamp":1637221737288,"user_tz":-540,"elapsed":13,"user":{"displayName":"뚜뚜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYh6WlXFq1X6XUJHBbCx-VfyZqesyHJAoYJep2Yw=s64","userId":"15837783100942680972"}},"outputId":"9ff28c60-1759-42cb-a6be-017e4345eecb"},"source":["print('='*100)\n","#x_test는, 4*4짜리 행렬데이터.\n","x_test = np.array([[4,4]], dtype = np.float32)\n","model_test = model_LinearRegression(x_test)\n","print(\"model for [4,4]: \", model_test.numpy()) #numpy()함수 통해, 출력에 적합하게 캐스팅\n","print('=' * 100)"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["====================================================================================================\n","model for [4,4]:  [[39.99069]]\n","====================================================================================================\n"]}]}]}