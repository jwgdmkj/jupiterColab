{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Relu",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOC1h490rjcrXGJNJonfxQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwgdmkj/jupiterColab/blob/master/Relu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KRNBRA7ZMJck",
        "outputId": "1e78b1e9-6555-470c-d26e-841ac9c762c3"
      },
      "source": [
        "#RELU가 아닌 것들: layer가 깊어질수록 학습이 안 됨\n",
        "#weight값을 변경시킬 때, 미분값을 이용하는데, sigmoid나 tanh 등은 갈수록 미분값이 0됨\n",
        "#RELU는 GRADIENT가 1로 유지되기에, 학습에 적절\n",
        "#이 RELU를 히든레이어에 추가하고자 한다.\n",
        "\n",
        "#linear regression: x y 데이터의 상관관계를 linear한 모델로 하는 것\n",
        "#최적화된 빨간 선을, keras로 구현\n",
        "import tensorflow as tf\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "tf.__version__\n",
        "\n",
        "dataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\n",
        "dataset_path\n",
        "\n",
        "# 받은 데이터는 csv형태로 저장됨.\n",
        "# pandas 형태로 csv파일을 읽음\n",
        "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
        "                'Acceleration', 'Model Year', 'Origin']\n",
        "raw_dataset = pd.read_csv(dataset_path, names=column_names,\n",
        "                      na_values = \"?\", comment='\\t',\n",
        "                      sep=\" \", skipinitialspace=True)\n",
        "\n",
        "dataset = raw_dataset.copy()\n",
        "dataset.tail() #dataset.head()\n",
        "\n",
        "dataset = dataset.dropna()\n",
        "\n",
        "#dataset.isna().sum()\n",
        "\n",
        "# len(dataset) #398개의 데이터가 준비됨\n",
        "# origin : 국가의 분류. 이는 필요하지 않은 data 이므로, 제거\n",
        "dataset.pop('Origin') \n",
        "\n",
        "#training set과 test_set을 분류 (overfitting 방지)\n",
        "train_dataset = dataset.sample(frac = 0.8, random_state = 0) #80%만 train에 사용\n",
        "test_dataset = dataset.drop(train_dataset.index)\n",
        "\n",
        "#모델 설계에 앞서, 데이터 정규화를 통해 편차를 감소시켜야 함\n",
        "#이때, MPG라는 값은 정규화가 불필요하기에, 새로운 '레이블'이란 이름으로 정의\n",
        "train_stats = train_dataset.describe()\n",
        "train_stats.pop('MPG')\n",
        "train_labels = train_dataset.pop('MPG')\n",
        "test_labels = test_dataset.pop('MPG')\n",
        "train_stats = train_stats.transpose()\n",
        "\n",
        "def norm(x):\n",
        "  return(x- train_stats['mean']) / train_stats['std']\n",
        "\n",
        "normed_train_data = norm(train_dataset)\n",
        "normed_test_data = norm(test_dataset)\n",
        "\n",
        "#x데이터 준비 완료. 여기에, linear 모델(x*행렬 w+bias 가 y값을 의미)\n",
        "#이 input data와 곱해질 행렬값을 학습하게 됨\n",
        "\n",
        "# 그 행렬 모델을 만든다\n",
        "# keras 모델이 알아야 할 것: input shape\n",
        "# 318, 6: 현재 data 수와, 칼럼의 수. 이 중 칼럼을 중시한다.\n",
        "inputs = keras.Input(shape=(normed_train_data.shape[1],)) # 6을 shape에 대입\n",
        "\n",
        "# 행렬 만들기.\n",
        "h= layers.Dense(64)(inputs) #64짜리 feature의 행렬 생성. \n",
        "# 최종 행렬은, y가 숫자 MPG값 1인 행렬을 맞춰야 하므로\n",
        "outputs = layers.Dense(1)(h)\n",
        "\n",
        "# 행렬 정의 완료. \n",
        "model = keras.Model(inputs = inputs, outputs = outputs)\n",
        "model.compile(loss = 'mse', optimizer = tf.keras.optimizers.RMSprop(0.001))\n",
        "\n",
        "\n",
        "#레이어 별로, input과 output 보기\n",
        "keras.utils.plot_model(model, \"my_first_model_with_shape_info.png\", show_shapes = True)\n",
        "\n",
        "# input과 output 사이의 히든레이어를 추가하고자 한다(현재는 1개)\n",
        "# h가, 다음 레이어의 input으로 들어가야 함.\n",
        "inputs = keras.Input(shape=(normed_train_data.shape[1],))\n",
        "h = layers.Dense(64)(inputs)\n",
        "#h= layers.Dense(64, activation='relu')(inputs)\n",
        "\n",
        "#Activation layer가 추가됨.\n",
        "#epoch을 하면, evaluation은 오차가 7가량까지 감소하며, \n",
        "#플러팅을 하면, 더 직선에 가까워짐\n",
        "h = layers.Activation('relu')(h)\n",
        "\n",
        "h = layers.Dense(32)(h)\n",
        "outputs = layers.Dense(1)(h)\n",
        "\n",
        "model = keras.Model(inputs = inputs, outputs = outputs)\n",
        "#결과, 레이어가 1개 증가\n",
        "model.compile(loss = 'mse', optimizer = tf.keras.optimizers.RMSprop(0.001))\n",
        "\n",
        "\"\"\"컴파일은, input 데이터를, y = x*W+b 하는 것\n",
        "이 떄, W와 b(bias)를 점차 변화시켜가며, 행렬값을, y를 실제 정답과 일치하도록 해야 함\n",
        "이 '오차를 줄여가는 과정'을 나아가는 것이 ML\n",
        "차이를 표현하는 방법 중, loss = 'mse'를 이용 & optimizer : 오차가 생겼을 떄, b를 어떻게 \n",
        "변경해갈 것인가? RMSprop을 이용\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "y = W*x+b의 W와 b는 현재 임의로 지정된 값. \n",
        "\"\"\"\n",
        "\"\"\"\n",
        "점차 학습시켜가며 똑똑하게 하자. 5개의 모델만을 test(with predict)\n",
        "\"\"\"\n",
        "example_batch = normed_train_data[:5]\n",
        "example_result = model.predict(example_batch) #부정확한 값. 이제, 학습시킨다\n",
        "\n",
        "# 이 모델을 훈련. fit 통해, 학습시켜간다(행렬값이바뀌며, y가 실제값에 가까워짐)\n",
        "#fit안에, x값(인풋데이터), 정답데이터(y), epoch삽입\n",
        "\"\"\"\n",
        "epoch: dataset 전체(318row)를, 각 한 번 학습하면 1. 이를 총 얼마나 학습할 것인가?\n",
        "트레이닝 하면, loss는 감소할 것. but 과적합의 문제: 무조건 똑똑해지면 안됨.\n",
        "어느 정도의 수준값까지만 똑똑해져야 함(validation loss). 학습에 쓰이지 않은 data.\n",
        "이 loss값을 보고, 과적합/아직 학습가능 여부 판단\n",
        "\"\"\"\n",
        "\n",
        "EPOCHS = 200\n",
        "history = model.fit(normed_train_data, train_labels, epochs = EPOCHS,\n",
        "                    validation_split = 0.2)\n",
        "\n",
        "\"\"\"\n",
        "오차 loss(트레이닝 로스), validation loss 전부 감소 중\n",
        "만일 이 EPCOH를 100으로 하면, 이 역시 더 작아지는데, 어느 순간부터 급격한 감소는 사라짐.\n",
        "이것을 가능케 하는 건 '케라스'로, VALIDATION LOSS를 자동으로 EARLY STOPPING통해\n",
        "트레이닝 멈추는 조건을 찾음\n",
        "\"\"\"\n",
        "\n",
        "# 오차 loss(트레이닝 로스), validation loss 전부 감소 중\\n만일 이 EPCOH를 100으로 하면, 이 역시 더 작아지는데,\n",
        " #어느 순간부터 급격한 감소는 사라짐.\\n이것을 가능케 하는 건 '케라스'로, VALIDATION LOSS를 자동으로 EARLY STOPPING통해\\n트레이닝 멈추는 조건을 찾음\\n\"\n",
        "\n",
        "\"\"\"\n",
        "PATIENCE=10은, VALIDATION LOSS가 10번 이상 줄지 않으면, 학습을 그만시킨단 뜻\n",
        "\"\"\"\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10)\n",
        "\n",
        "model.fit(normed_train_data, train_labels, epochs = EPOCHS, validation_split = 0.2,\n",
        "          callbacks = [early_stop])\n",
        "\n",
        "\"\"\"\n",
        "이제, 평가를 하고자함. 그 결과, 맨 초기의 약 500에서 10.91까지 감소\n",
        "\"\"\"\n",
        "loss = model.evaluate(normed_test_data, test_labels, verbose=2)\n",
        "\n",
        "\"\"\"\n",
        "실제 값 예측 법: predict로\n",
        "\"\"\"\n",
        "test_prediction= model.predict(normed_test_data).flatten()\n",
        "\n",
        "#실제값과 예측값을 비교.\n",
        "plt.scatter(test_labels, test_prediction)\n",
        "plt.xlabel('TRUE Values [MPG]')\n",
        "plt.ylabel('Prediction [MPG]')\n",
        "plt.axis('equal')\n",
        "plt.axis('square')\n",
        "plt.xlim([0, plt.xlim()[1]])\n",
        "plt.ylim([0, plt.ylim()[1]])\n",
        "_ = plt.plot([-100, 100], [-100, 100])\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "8/8 [==============================] - 1s 20ms/step - loss: 568.8103 - val_loss: 553.8718\n",
            "Epoch 2/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 508.2822 - val_loss: 494.3649\n",
            "Epoch 3/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 452.9648 - val_loss: 432.3276\n",
            "Epoch 4/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 394.9819 - val_loss: 368.1646\n",
            "Epoch 5/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 334.7748 - val_loss: 302.6211\n",
            "Epoch 6/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 273.9938 - val_loss: 238.9703\n",
            "Epoch 7/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 215.7854 - val_loss: 182.0486\n",
            "Epoch 8/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 163.0822 - val_loss: 132.4566\n",
            "Epoch 9/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 118.4436 - val_loss: 95.3209\n",
            "Epoch 10/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 84.4852 - val_loss: 70.0147\n",
            "Epoch 11/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 61.1055 - val_loss: 56.2866\n",
            "Epoch 12/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 48.1145 - val_loss: 50.4246\n",
            "Epoch 13/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 42.1579 - val_loss: 46.8136\n",
            "Epoch 14/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 38.3832 - val_loss: 43.3538\n",
            "Epoch 15/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 34.7834 - val_loss: 39.6035\n",
            "Epoch 16/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 31.7540 - val_loss: 35.1428\n",
            "Epoch 17/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 28.6219 - val_loss: 31.6217\n",
            "Epoch 18/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 25.9313 - val_loss: 28.6205\n",
            "Epoch 19/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 23.2872 - val_loss: 25.4979\n",
            "Epoch 20/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 20.9890 - val_loss: 24.5325\n",
            "Epoch 21/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 19.3665 - val_loss: 20.8101\n",
            "Epoch 22/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 17.6963 - val_loss: 18.9231\n",
            "Epoch 23/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16.3759 - val_loss: 17.6758\n",
            "Epoch 24/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 15.2416 - val_loss: 17.6902\n",
            "Epoch 25/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 14.4279 - val_loss: 15.9188\n",
            "Epoch 26/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 13.7251 - val_loss: 14.9184\n",
            "Epoch 27/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 13.0217 - val_loss: 13.8069\n",
            "Epoch 28/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.5017 - val_loss: 13.6374\n",
            "Epoch 29/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.0839 - val_loss: 12.7242\n",
            "Epoch 30/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11.7394 - val_loss: 13.2121\n",
            "Epoch 31/200\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 11.7151 - val_loss: 12.3788\n",
            "Epoch 32/200\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 11.1585 - val_loss: 12.1697\n",
            "Epoch 33/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 10.8718 - val_loss: 11.8708\n",
            "Epoch 34/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10.6411 - val_loss: 11.8004\n",
            "Epoch 35/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 10.2502 - val_loss: 11.6272\n",
            "Epoch 36/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10.2469 - val_loss: 11.9526\n",
            "Epoch 37/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10.3231 - val_loss: 10.9802\n",
            "Epoch 38/200\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 9.8303 - val_loss: 10.9735\n",
            "Epoch 39/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 9.4535 - val_loss: 11.2505\n",
            "Epoch 40/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9.7680 - val_loss: 10.3568\n",
            "Epoch 41/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 9.4483 - val_loss: 10.4578\n",
            "Epoch 42/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9.1652 - val_loss: 10.3898\n",
            "Epoch 43/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9.1912 - val_loss: 10.7056\n",
            "Epoch 44/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9.1428 - val_loss: 10.2261\n",
            "Epoch 45/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8.9426 - val_loss: 10.4491\n",
            "Epoch 46/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 8.8672 - val_loss: 10.4303\n",
            "Epoch 47/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 8.8121 - val_loss: 10.0562\n",
            "Epoch 48/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8.6138 - val_loss: 10.0654\n",
            "Epoch 49/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8.4563 - val_loss: 9.8351\n",
            "Epoch 50/200\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8.3652 - val_loss: 9.9919\n",
            "Epoch 51/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8.1950 - val_loss: 9.9992\n",
            "Epoch 52/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8.2317 - val_loss: 10.2589\n",
            "Epoch 53/200\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 8.2638 - val_loss: 10.0177\n",
            "Epoch 54/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8.3085 - val_loss: 9.6326\n",
            "Epoch 55/200\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8.0101 - val_loss: 9.7585\n",
            "Epoch 56/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8.0077 - val_loss: 10.2946\n",
            "Epoch 57/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.9646 - val_loss: 9.6994\n",
            "Epoch 58/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 7.9258 - val_loss: 9.7843\n",
            "Epoch 59/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.8140 - val_loss: 9.7485\n",
            "Epoch 60/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.8866 - val_loss: 9.4739\n",
            "Epoch 61/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7.9677 - val_loss: 9.6287\n",
            "Epoch 62/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7.8780 - val_loss: 9.4066\n",
            "Epoch 63/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7.6132 - val_loss: 9.8105\n",
            "Epoch 64/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.6897 - val_loss: 9.6310\n",
            "Epoch 65/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.5124 - val_loss: 9.3319\n",
            "Epoch 66/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.4814 - val_loss: 9.3767\n",
            "Epoch 67/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.7075 - val_loss: 9.4813\n",
            "Epoch 68/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.5945 - val_loss: 9.3271\n",
            "Epoch 69/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 7.4244 - val_loss: 9.1607\n",
            "Epoch 70/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.4315 - val_loss: 9.2225\n",
            "Epoch 71/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.3156 - val_loss: 9.7435\n",
            "Epoch 72/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.3515 - val_loss: 9.2883\n",
            "Epoch 73/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7.3133 - val_loss: 9.2151\n",
            "Epoch 74/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 7.3799 - val_loss: 9.2784\n",
            "Epoch 75/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.2928 - val_loss: 9.0975\n",
            "Epoch 76/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.2747 - val_loss: 9.4493\n",
            "Epoch 77/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.2796 - val_loss: 9.2035\n",
            "Epoch 78/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 7.2034 - val_loss: 9.0669\n",
            "Epoch 79/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.3401 - val_loss: 9.1441\n",
            "Epoch 80/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.1286 - val_loss: 9.1158\n",
            "Epoch 81/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7.0181 - val_loss: 8.9517\n",
            "Epoch 82/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.2303 - val_loss: 9.0365\n",
            "Epoch 83/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7.0110 - val_loss: 9.2251\n",
            "Epoch 84/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.1068 - val_loss: 9.1006\n",
            "Epoch 85/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.9334 - val_loss: 9.1082\n",
            "Epoch 86/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.8519 - val_loss: 10.0736\n",
            "Epoch 87/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7.2059 - val_loss: 8.9902\n",
            "Epoch 88/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.9661 - val_loss: 8.8937\n",
            "Epoch 89/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.9645 - val_loss: 9.5546\n",
            "Epoch 90/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.9341 - val_loss: 9.5370\n",
            "Epoch 91/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.1398 - val_loss: 8.9623\n",
            "Epoch 92/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.8712 - val_loss: 9.0367\n",
            "Epoch 93/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.9073 - val_loss: 9.3897\n",
            "Epoch 94/200\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 6.8488 - val_loss: 9.0246\n",
            "Epoch 95/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.0300 - val_loss: 8.9354\n",
            "Epoch 96/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.8854 - val_loss: 8.9103\n",
            "Epoch 97/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.8250 - val_loss: 8.9434\n",
            "Epoch 98/200\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 6.9089 - val_loss: 8.9817\n",
            "Epoch 99/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.8891 - val_loss: 9.0325\n",
            "Epoch 100/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.9126 - val_loss: 8.9836\n",
            "Epoch 101/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.7262 - val_loss: 9.4431\n",
            "Epoch 102/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.6140 - val_loss: 9.0412\n",
            "Epoch 103/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.7495 - val_loss: 8.6912\n",
            "Epoch 104/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.9900 - val_loss: 8.8573\n",
            "Epoch 105/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.7049 - val_loss: 8.7733\n",
            "Epoch 106/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.6862 - val_loss: 9.0911\n",
            "Epoch 107/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.8985 - val_loss: 8.8416\n",
            "Epoch 108/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.7358 - val_loss: 8.9187\n",
            "Epoch 109/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.6056 - val_loss: 9.1651\n",
            "Epoch 110/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.5314 - val_loss: 9.1000\n",
            "Epoch 111/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.5136 - val_loss: 8.9742\n",
            "Epoch 112/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.6749 - val_loss: 9.1893\n",
            "Epoch 113/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.6740 - val_loss: 8.9522\n",
            "Epoch 114/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 6.7346 - val_loss: 9.1984\n",
            "Epoch 115/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.6215 - val_loss: 8.9227\n",
            "Epoch 116/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.7528 - val_loss: 9.1663\n",
            "Epoch 117/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.5771 - val_loss: 9.1401\n",
            "Epoch 118/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.6895 - val_loss: 9.0646\n",
            "Epoch 119/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.5077 - val_loss: 8.8058\n",
            "Epoch 120/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.4958 - val_loss: 9.2701\n",
            "Epoch 121/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.5829 - val_loss: 9.0465\n",
            "Epoch 122/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.5847 - val_loss: 9.2213\n",
            "Epoch 123/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.6082 - val_loss: 8.9585\n",
            "Epoch 124/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.5801 - val_loss: 9.0655\n",
            "Epoch 125/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.3944 - val_loss: 9.0121\n",
            "Epoch 126/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.4234 - val_loss: 9.7817\n",
            "Epoch 127/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.5761 - val_loss: 9.2602\n",
            "Epoch 128/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.3489 - val_loss: 8.9678\n",
            "Epoch 129/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.4520 - val_loss: 9.2732\n",
            "Epoch 130/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.4343 - val_loss: 9.0126\n",
            "Epoch 131/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.4574 - val_loss: 9.4153\n",
            "Epoch 132/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.5690 - val_loss: 9.2623\n",
            "Epoch 133/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.3173 - val_loss: 9.1043\n",
            "Epoch 134/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.6204 - val_loss: 8.9841\n",
            "Epoch 135/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.3461 - val_loss: 9.0937\n",
            "Epoch 136/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.4588 - val_loss: 9.0970\n",
            "Epoch 137/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.4335 - val_loss: 9.3783\n",
            "Epoch 138/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.3816 - val_loss: 9.1496\n",
            "Epoch 139/200\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 6.3148 - val_loss: 9.2140\n",
            "Epoch 140/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.2596 - val_loss: 9.2220\n",
            "Epoch 141/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.4001 - val_loss: 9.0954\n",
            "Epoch 142/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.3131 - val_loss: 9.4217\n",
            "Epoch 143/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.2317 - val_loss: 9.2535\n",
            "Epoch 144/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.4171 - val_loss: 9.1257\n",
            "Epoch 145/200\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 6.2777 - val_loss: 9.7918\n",
            "Epoch 146/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.3106 - val_loss: 9.1124\n",
            "Epoch 147/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.2642 - val_loss: 9.1787\n",
            "Epoch 148/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.2104 - val_loss: 9.1590\n",
            "Epoch 149/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.3059 - val_loss: 9.2302\n",
            "Epoch 150/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.2498 - val_loss: 9.1122\n",
            "Epoch 151/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.2681 - val_loss: 9.0611\n",
            "Epoch 152/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.2358 - val_loss: 9.6273\n",
            "Epoch 153/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.3792 - val_loss: 9.2489\n",
            "Epoch 154/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.1865 - val_loss: 9.5215\n",
            "Epoch 155/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.2364 - val_loss: 9.0770\n",
            "Epoch 156/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.3690 - val_loss: 9.2436\n",
            "Epoch 157/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.0812 - val_loss: 9.7159\n",
            "Epoch 158/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.1262 - val_loss: 9.3723\n",
            "Epoch 159/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.4351 - val_loss: 9.3147\n",
            "Epoch 160/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.3381 - val_loss: 9.0976\n",
            "Epoch 161/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.1434 - val_loss: 9.1502\n",
            "Epoch 162/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.1466 - val_loss: 9.2107\n",
            "Epoch 163/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.2996 - val_loss: 9.0887\n",
            "Epoch 164/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.1773 - val_loss: 9.1733\n",
            "Epoch 165/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.0486 - val_loss: 9.2255\n",
            "Epoch 166/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.3015 - val_loss: 9.1890\n",
            "Epoch 167/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.0500 - val_loss: 10.0730\n",
            "Epoch 168/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.1984 - val_loss: 9.0610\n",
            "Epoch 169/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.1307 - val_loss: 9.0888\n",
            "Epoch 170/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.1383 - val_loss: 9.0447\n",
            "Epoch 171/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.0655 - val_loss: 9.1939\n",
            "Epoch 172/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.1943 - val_loss: 9.4046\n",
            "Epoch 173/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 6.0099 - val_loss: 9.3664\n",
            "Epoch 174/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.0845 - val_loss: 9.2028\n",
            "Epoch 175/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 6.0506 - val_loss: 9.4795\n",
            "Epoch 176/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.2230 - val_loss: 9.2131\n",
            "Epoch 177/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 6.1846 - val_loss: 9.2934\n",
            "Epoch 178/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9531 - val_loss: 9.2628\n",
            "Epoch 179/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9895 - val_loss: 9.1785\n",
            "Epoch 180/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.1339 - val_loss: 9.1395\n",
            "Epoch 181/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9595 - val_loss: 9.2120\n",
            "Epoch 182/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.0244 - val_loss: 9.4089\n",
            "Epoch 183/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 6.1452 - val_loss: 9.5772\n",
            "Epoch 184/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.0267 - val_loss: 9.4043\n",
            "Epoch 185/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.1113 - val_loss: 9.4275\n",
            "Epoch 186/200\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.9660 - val_loss: 9.3569\n",
            "Epoch 187/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.1541 - val_loss: 9.7218\n",
            "Epoch 188/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 5.8966 - val_loss: 9.2713\n",
            "Epoch 189/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9238 - val_loss: 9.6590\n",
            "Epoch 190/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.9995 - val_loss: 9.3740\n",
            "Epoch 191/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.1241 - val_loss: 9.3760\n",
            "Epoch 192/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.0054 - val_loss: 9.3796\n",
            "Epoch 193/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8384 - val_loss: 9.9023\n",
            "Epoch 194/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.0744 - val_loss: 9.3599\n",
            "Epoch 195/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.2020 - val_loss: 9.3654\n",
            "Epoch 196/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9016 - val_loss: 9.4949\n",
            "Epoch 197/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9133 - val_loss: 9.6099\n",
            "Epoch 198/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.1467 - val_loss: 9.2785\n",
            "Epoch 199/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8733 - val_loss: 9.4125\n",
            "Epoch 200/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8142 - val_loss: 10.1907\n",
            "Epoch 1/200\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 6.0120 - val_loss: 10.4363\n",
            "Epoch 2/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9123 - val_loss: 9.5248\n",
            "Epoch 3/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9444 - val_loss: 9.2042\n",
            "Epoch 4/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 5.8493 - val_loss: 10.3931\n",
            "Epoch 5/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.0808 - val_loss: 10.0422\n",
            "Epoch 6/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.0514 - val_loss: 9.4326\n",
            "Epoch 7/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 5.9031 - val_loss: 9.2998\n",
            "Epoch 8/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7711 - val_loss: 9.4660\n",
            "Epoch 9/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8474 - val_loss: 9.7057\n",
            "Epoch 10/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 5.8377 - val_loss: 9.3628\n",
            "Epoch 11/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8331 - val_loss: 9.4237\n",
            "Epoch 12/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.9642 - val_loss: 9.3817\n",
            "Epoch 13/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.9999 - val_loss: 9.4407\n",
            "3/3 - 0s - loss: 7.0911\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}